obiwan
pid: 3799951
conda env: greaselm
screen: 

gpu: 0,1

Namespace(att_head_num=2, batch_size=128, cuda=True, dataset='nephqa', debug=False, decoder_lr=0.001, dev_adj='data/nephqa/graph/dev.graph.adj.pk', dev_statements='data/nephqa/statement/dev.statement.jsonl', drop_partial_batch=False, dropoutf=0.2, dropoutg=0.2, dropouti=0.2, encoder='roberta-large', encoder_layer=-1, encoder_lr=1e-05, ent_emb=['tzw'], ent_emb_paths=['data/cpnet/tzw.ent.npy'], eval_batch_size=2, fc_dim=200, fc_layer_num=0, fill_partial_batch=False, fp16=True, freeze_ent_emb=True, gnn_dim=200, inhouse=False, inhouse_train_qids='data/nephqa/inhouse_split_qids.txt', init_range=0.02, k=5, load_model_path=None, log_interval=10, loss='cross_entropy', lr_schedule='fixed', max_epochs_before_stop=50, max_grad_norm=1.0, max_node_num=200, max_seq_len=100, mini_batch_size=1, mode='train', n_epochs=100, num_relation=38, optim='radam', refreeze_epoch=10000, save_dir='saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753', save_model=True, seed=0, simple=False, subsample=1.0, test_adj='data/nephqa/graph/test.graph.adj.pk', test_statements='data/nephqa/statement/test.statement.jsonl', train_adj='data/nephqa/graph/train.graph.adj.pk', train_statements='data/nephqa/statement/train.statement.jsonl', unfreeze_epoch=4, use_cache=True, warmup_steps=150, weight_decay=0.01)
| num_concepts: 799273 |
train_statement_path data/nephqa/statement/train.statement.jsonl
num_choice 5
| ori_adj_len: mu 2513.54 sigma 1590.31 | adj_len: 191.99 | prune_rate： 0.94 | qc_num: 35.95 | ac_num: 5.21 |
| ori_adj_len: mu 2705.23 sigma 1638.22 | adj_len: 193.85 | prune_rate： 0.95 | qc_num: 38.70 | ac_num: 5.55 |
| ori_adj_len: mu 2566.88 sigma 1535.80 | adj_len: 195.00 | prune_rate： 0.96 | qc_num: 36.10 | ac_num: 5.09 |
args.num_relation 38
parameters:
	concept_emb.emb.weight                       	fixed	torch.Size([799273, 1024])	device:cuda:1
	concept_emb.cpt_transform.weight             	trainable	torch.Size([200, 1024])	device:cuda:1
	concept_emb.cpt_transform.bias               	trainable	torch.Size([200])	device:cuda:1
	svec2nvec.weight                             	trainable	torch.Size([200, 1024])	device:cuda:1
	svec2nvec.bias                               	trainable	torch.Size([200])	device:cuda:1
	gnn.emb_node_type.weight                     	trainable	torch.Size([100, 4])	device:cuda:1
	gnn.emb_node_type.bias                       	trainable	torch.Size([100])	device:cuda:1
	gnn.emb_score.weight                         	trainable	torch.Size([100, 100])	device:cuda:1
	gnn.emb_score.bias                           	trainable	torch.Size([100])	device:cuda:1
	gnn.edge_encoder.0.weight                    	trainable	torch.Size([200, 47])	device:cuda:1
	gnn.edge_encoder.0.bias                      	trainable	torch.Size([200])	device:cuda:1
	gnn.edge_encoder.1.weight                    	trainable	torch.Size([200])	device:cuda:1
	gnn.edge_encoder.1.bias                      	trainable	torch.Size([200])	device:cuda:1
	gnn.edge_encoder.3.weight                    	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.edge_encoder.3.bias                      	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.linear_key.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.0.linear_key.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.linear_msg.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.0.linear_msg.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.linear_query.weight         	trainable	torch.Size([200, 400])	device:cuda:1
	gnn.gnn_layers.0.linear_query.bias           	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.mlp.0.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.0.mlp.0.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.mlp.1.weight                	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.mlp.1.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.0.mlp.3.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.0.mlp.3.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.linear_key.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.1.linear_key.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.linear_msg.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.1.linear_msg.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.linear_query.weight         	trainable	torch.Size([200, 400])	device:cuda:1
	gnn.gnn_layers.1.linear_query.bias           	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.mlp.0.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.1.mlp.0.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.mlp.1.weight                	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.mlp.1.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.1.mlp.3.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.1.mlp.3.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.linear_key.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.2.linear_key.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.linear_msg.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.2.linear_msg.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.linear_query.weight         	trainable	torch.Size([200, 400])	device:cuda:1
	gnn.gnn_layers.2.linear_query.bias           	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.mlp.0.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.2.mlp.0.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.mlp.1.weight                	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.mlp.1.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.2.mlp.3.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.2.mlp.3.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.linear_key.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.3.linear_key.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.linear_msg.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.3.linear_msg.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.linear_query.weight         	trainable	torch.Size([200, 400])	device:cuda:1
	gnn.gnn_layers.3.linear_query.bias           	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.mlp.0.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.3.mlp.0.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.mlp.1.weight                	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.mlp.1.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.3.mlp.3.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.3.mlp.3.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.linear_key.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.4.linear_key.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.linear_msg.weight           	trainable	torch.Size([200, 600])	device:cuda:1
	gnn.gnn_layers.4.linear_msg.bias             	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.linear_query.weight         	trainable	torch.Size([200, 400])	device:cuda:1
	gnn.gnn_layers.4.linear_query.bias           	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.mlp.0.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.4.mlp.0.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.mlp.1.weight                	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.mlp.1.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.gnn_layers.4.mlp.3.weight                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.gnn_layers.4.mlp.3.bias                  	trainable	torch.Size([200])	device:cuda:1
	gnn.Vh.weight                                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.Vh.bias                                  	trainable	torch.Size([200])	device:cuda:1
	gnn.Vx.weight                                	trainable	torch.Size([200, 200])	device:cuda:1
	gnn.Vx.bias                                  	trainable	torch.Size([200])	device:cuda:1
	pooler.w_qs.weight                           	trainable	torch.Size([200, 1024])	device:cuda:1
	pooler.w_qs.bias                             	trainable	torch.Size([200])	device:cuda:1
	pooler.w_ks.weight                           	trainable	torch.Size([200, 200])	device:cuda:1
	pooler.w_ks.bias                             	trainable	torch.Size([200])	device:cuda:1
	pooler.w_vs.weight                           	trainable	torch.Size([200, 200])	device:cuda:1
	pooler.w_vs.bias                             	trainable	torch.Size([200])	device:cuda:1
	fc.layers.0-Linear.weight                    	trainable	torch.Size([1, 1424])	device:cuda:1
	fc.layers.0-Linear.bias                      	trainable	torch.Size([1])	device:cuda:1
	total: 2845025

-----------------------------------------------------------------------
Using fp16 training
-----------------------------------------------------------------------
| epoch   0 | step     9 | dev_acc  0.1475 | test_acc  0.2093 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.0
| step     9 |  lr: 0.0000100 | loss  1.6184 | ms/batch 2072.70 |
-----------------------------------------------------------------------
| epoch   1 | step    18 | dev_acc  0.1967 | test_acc  0.1860 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.1
| step    19 |  lr: 0.0000100 | loss  1.6163 | ms/batch 4207.35 |
-----------------------------------------------------------------------
| epoch   2 | step    27 | dev_acc  0.2213 | test_acc  0.2326 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.2
| step    29 |  lr: 0.0000100 | loss  1.6121 | ms/batch 6284.45 |
-----------------------------------------------------------------------
| epoch   3 | step    36 | dev_acc  0.1557 | test_acc  0.2233 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.3
| step    39 |  lr: 0.0000100 | loss  1.6147 | ms/batch 10325.37 |
-----------------------------------------------------------------------
| epoch   4 | step    45 | dev_acc  0.1230 | test_acc  0.2326 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.4
| step    49 |  lr: 0.0000100 | loss  1.6170 | ms/batch 12766.78 |
-----------------------------------------------------------------------
| epoch   5 | step    54 | dev_acc  0.1393 | test_acc  0.2558 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.5
| step    59 |  lr: 0.0000100 | loss  1.6089 | ms/batch 15392.63 |
-----------------------------------------------------------------------
| epoch   6 | step    63 | dev_acc  0.1311 | test_acc  0.2140 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.6
| step    69 |  lr: 0.0000100 | loss  1.6113 | ms/batch 18044.26 |
-----------------------------------------------------------------------
| epoch   7 | step    72 | dev_acc  0.1639 | test_acc  0.2093 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.7
| step    79 |  lr: 0.0000100 | loss  1.6111 | ms/batch 20586.99 |
-----------------------------------------------------------------------
| epoch   8 | step    81 | dev_acc  0.1311 | test_acc  0.2233 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.8
| step    89 |  lr: 0.0000100 | loss  1.6050 | ms/batch 21940.40 |
-----------------------------------------------------------------------
| epoch   9 | step    90 | dev_acc  0.1475 | test_acc  0.2372 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.9
-----------------------------------------------------------------------
| epoch  10 | step    99 | dev_acc  0.1393 | test_acc  0.2465 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.10
| step    99 |  lr: 0.0000100 | loss  1.5989 | ms/batch 2480.35 |
-----------------------------------------------------------------------
| epoch  11 | step   108 | dev_acc  0.1721 | test_acc  0.2000 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.11
| step   109 |  lr: 0.0000100 | loss  1.5995 | ms/batch 5005.13 |
-----------------------------------------------------------------------
| epoch  12 | step   117 | dev_acc  0.1639 | test_acc  0.2326 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.12
| step   119 |  lr: 0.0000100 | loss  1.6064 | ms/batch 7672.94 |
-----------------------------------------------------------------------
| epoch  13 | step   126 | dev_acc  0.1393 | test_acc  0.2233 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.13
| step   129 |  lr: 0.0000100 | loss  1.6032 | ms/batch 10262.65 |
-----------------------------------------------------------------------
| epoch  14 | step   135 | dev_acc  0.1311 | test_acc  0.2372 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.14
| step   139 |  lr: 0.0000100 | loss  1.6083 | ms/batch 12793.51 |
-----------------------------------------------------------------------
| epoch  15 | step   144 | dev_acc  0.1639 | test_acc  0.2372 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.15
| step   149 |  lr: 0.0000100 | loss  1.5939 | ms/batch 15498.59 |
-----------------------------------------------------------------------
| epoch  16 | step   153 | dev_acc  0.1721 | test_acc  0.2279 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.16
| step   159 |  lr: 0.0000100 | loss  1.6092 | ms/batch 17983.65 |
-----------------------------------------------------------------------
| epoch  17 | step   162 | dev_acc  0.1639 | test_acc  0.2465 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.17
| step   169 |  lr: 0.0000100 | loss  1.5856 | ms/batch 20602.65 |
-----------------------------------------------------------------------
| epoch  18 | step   171 | dev_acc  0.1557 | test_acc  0.2326 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.18
| step   179 |  lr: 0.0000100 | loss  1.5889 | ms/batch 21911.50 |
-----------------------------------------------------------------------
| epoch  19 | step   180 | dev_acc  0.1885 | test_acc  0.2279 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.19
-----------------------------------------------------------------------
| epoch  20 | step   189 | dev_acc  0.2131 | test_acc  0.2419 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.20
| step   189 |  lr: 0.0000100 | loss  1.5801 | ms/batch 2566.08 |
-----------------------------------------------------------------------
| epoch  21 | step   198 | dev_acc  0.1967 | test_acc  0.2140 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.21
| step   199 |  lr: 0.0000100 | loss  1.5686 | ms/batch 5291.42 |
-----------------------------------------------------------------------
| epoch  22 | step   207 | dev_acc  0.1639 | test_acc  0.2093 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.22
| step   209 |  lr: 0.0000100 | loss  1.5568 | ms/batch 7658.10 |
-----------------------------------------------------------------------
| epoch  23 | step   216 | dev_acc  0.1885 | test_acc  0.1953 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.23
| step   219 |  lr: 0.0000100 | loss  1.5499 | ms/batch 10362.03 |
-----------------------------------------------------------------------
| epoch  24 | step   225 | dev_acc  0.1885 | test_acc  0.2186 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.24
| step   229 |  lr: 0.0000100 | loss  1.5349 | ms/batch 12912.59 |
-----------------------------------------------------------------------
| epoch  25 | step   234 | dev_acc  0.2295 | test_acc  0.2047 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.25
| step   239 |  lr: 0.0000100 | loss  1.5110 | ms/batch 15503.84 |
-----------------------------------------------------------------------
| epoch  26 | step   243 | dev_acc  0.2131 | test_acc  0.2233 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.26
| step   249 |  lr: 0.0000100 | loss  1.4906 | ms/batch 17943.41 |
-----------------------------------------------------------------------
| epoch  27 | step   252 | dev_acc  0.1967 | test_acc  0.2233 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.27
| step   259 |  lr: 0.0000100 | loss  1.4567 | ms/batch 20654.75 |
-----------------------------------------------------------------------
| epoch  28 | step   261 | dev_acc  0.2623 | test_acc  0.2186 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.28
| step   269 |  lr: 0.0000100 | loss  1.4233 | ms/batch 21951.60 |
-----------------------------------------------------------------------
| epoch  29 | step   270 | dev_acc  0.2869 | test_acc  0.2558 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.29
-----------------------------------------------------------------------
| epoch  30 | step   279 | dev_acc  0.3197 | test_acc  0.2419 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.30
| step   279 |  lr: 0.0000100 | loss  1.3929 | ms/batch 2551.79 |
-----------------------------------------------------------------------
| epoch  31 | step   288 | dev_acc  0.2869 | test_acc  0.2558 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.31
| step   289 |  lr: 0.0000100 | loss  1.3646 | ms/batch 5115.64 |
-----------------------------------------------------------------------
| epoch  32 | step   297 | dev_acc  0.3033 | test_acc  0.2512 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.32
| step   299 |  lr: 0.0000100 | loss  1.2808 | ms/batch 7640.18 |
-----------------------------------------------------------------------
| epoch  33 | step   306 | dev_acc  0.3607 | test_acc  0.2884 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.33
| step   309 |  lr: 0.0000100 | loss  1.2179 | ms/batch 10121.33 |
-----------------------------------------------------------------------
| epoch  34 | step   315 | dev_acc  0.3525 | test_acc  0.2698 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.34
| step   319 |  lr: 0.0000100 | loss  1.1778 | ms/batch 12874.89 |
-----------------------------------------------------------------------
| epoch  35 | step   324 | dev_acc  0.3525 | test_acc  0.2930 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.35
| step   329 |  lr: 0.0000100 | loss  1.0997 | ms/batch 15478.38 |
-----------------------------------------------------------------------
| epoch  36 | step   333 | dev_acc  0.3361 | test_acc  0.2698 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.36
| step   339 |  lr: 0.0000100 | loss  1.0618 | ms/batch 18100.25 |
-----------------------------------------------------------------------
| epoch  37 | step   342 | dev_acc  0.3361 | test_acc  0.3023 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.37
| step   349 |  lr: 0.0000100 | loss  0.9878 | ms/batch 20579.20 |
-----------------------------------------------------------------------
| epoch  38 | step   351 | dev_acc  0.3525 | test_acc  0.2930 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.38
| step   359 |  lr: 0.0000100 | loss  0.9079 | ms/batch 21947.83 |
-----------------------------------------------------------------------
| epoch  39 | step   360 | dev_acc  0.3525 | test_acc  0.2930 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.39
-----------------------------------------------------------------------
| epoch  40 | step   369 | dev_acc  0.3689 | test_acc  0.3209 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.40
| step   369 |  lr: 0.0000100 | loss  0.8679 | ms/batch 2500.17 |
-----------------------------------------------------------------------
| epoch  41 | step   378 | dev_acc  0.3197 | test_acc  0.3116 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.41
| step   379 |  lr: 0.0000100 | loss  0.8459 | ms/batch 5099.19 |
-----------------------------------------------------------------------
| epoch  42 | step   387 | dev_acc  0.3361 | test_acc  0.2791 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.42
| step   389 |  lr: 0.0000100 | loss  0.7268 | ms/batch 7758.35 |
-----------------------------------------------------------------------
| epoch  43 | step   396 | dev_acc  0.3197 | test_acc  0.2884 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.43
| step   399 |  lr: 0.0000100 | loss  0.6914 | ms/batch 10233.86 |
-----------------------------------------------------------------------
| epoch  44 | step   405 | dev_acc  0.3115 | test_acc  0.2698 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.44
| step   409 |  lr: 0.0000100 | loss  0.6191 | ms/batch 12921.16 |
-----------------------------------------------------------------------
| epoch  45 | step   414 | dev_acc  0.3115 | test_acc  0.2837 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.45
| step   419 |  lr: 0.0000100 | loss  0.5824 | ms/batch 15478.46 |
-----------------------------------------------------------------------
| epoch  46 | step   423 | dev_acc  0.2951 | test_acc  0.3116 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.46
| step   429 |  lr: 0.0000100 | loss  0.5287 | ms/batch 17829.86 |
-----------------------------------------------------------------------
| epoch  47 | step   432 | dev_acc  0.3443 | test_acc  0.3023 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.47
| step   439 |  lr: 0.0000100 | loss  0.4769 | ms/batch 20502.94 |
-----------------------------------------------------------------------
| epoch  48 | step   441 | dev_acc  0.3443 | test_acc  0.2977 |
-----------------------------------------------------------------------
model saved to saved_models/nephqa/enc-roberta-large__k5__gnndim200__bs128__seed0__20220609_204753/model.pt.48
